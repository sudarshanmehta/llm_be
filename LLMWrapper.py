# -*- coding: utf-8 -*-
"""LLMWrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a4Gy9Gvwox96YB3_vp-kWq5NwZ5emFso
"""

import os
import sys
import types
import json
import argparse
import importlib

import concurrent.futures
import multiprocessing

# from BaseLLM import *

def task(model_name,prompt,taskid = None, convid = None):
    model = LLMWrapper.model_registry[model_name]
    if not model:
        return
    response, is_code = model.get_response(prompt, taskid, convid)
    return model_name, response, is_code

class LLMWrapper(object):
  model_registry = {}
  config_file = None

  def __init__(self, config=None, models=[], model_names=[]):

      self.models = models
      self.model_names = model_names
      LLMWrapper.config_file = config

      #registry = Models.model_registry
      if config:
          self.models = self.load_models( config) #reads data
  def load_models(self, config):
          """ Load Models from a config file """

          # Read Config file
          conf_data = LLMWrapper.read_config(config)

          loaded_llms = {}
          if conf_data is not None:

              try:
                  llms = conf_data["Config"]["LLMWrapper"]["llms"]
                  if self.model_names:
                      llms = [item for item in llms if item["class_name"] in self.model_names]
                  for llm in llms:
                      model_file = None
                      model = None
                      class_name = None
                      try:
                          model_file = llm['file']
                          model =  llm['model']
                          class_name = llm['class_name']
                      except:
                          print('ERROR (LLMWrapper): could not parse llm {0} from config, skipping' .format(llm))
                          pass

                      # loading model file

                      # Check if full path of file exists, else see if path relative to package-source exists
                      if not os.path.exists(os.path.abspath(model_file)):
                          rel_path, cfile = script_path = os.path.split(os.path.abspath(__file__))
                          mfile = os.path.join(rel_path, model_file)
                      else:
                          mfile = model_file
                      head , tail = os.path.split(os.path.abspath(mfile))
                      sys.path.append(head)
                      model_file_name = os.path.splitext(tail)[0]
                      print('loading module {0}...' .format(os.path.abspath(mfile)))
                      try:
                          llm_model = importlib.import_module(model_file_name)
                          loaded_llms[model_file_name] = llm_model
                          print('finished loading module {0}' .format(model_file_name))
                          # Add model name to class var
                          if class_name not in self.model_names:
                            self.model_names.append(class_name)
                            self.register_model(llm_model, llm)
                      except Exception as e:
                          print("ERROR (LLMWrapper): could not load model from config file '{0}' : {1}, skipping".format(model_file_name, str(e)))

              except Exception as e:
                  print("could not read llms from config file {0} : {1}" .format(config, str(e)))
                  return
          if len(loaded_llms):
            print('loaded llms: {0}' .format(loaded_llms))

          return

  def __str__(self):
    return f"LLM fine-tuning with {self.models}"

